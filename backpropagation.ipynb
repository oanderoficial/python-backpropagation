{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953ae541-349c-4120-9cc6-e76ea3483b1c",
   "metadata": {},
   "source": [
    "# Python-backpropagation\n",
    "\n",
    "## Exemplo básico de backpropagation em Python usando NumPy\n",
    "\n",
    "* Usando backpropagation para treinar uma rede neural simples a resolver o problema XOR. A rede aprende ajustando os pesos automaticamente com base no erro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b0fbc-40f4-413f-94cc-ad80b707e804",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Funções de ativação: Tanh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a9c6e-0008-467b-b597-0dafc02faf83",
   "metadata": {},
   "source": [
    "## O problema XOR\n",
    "\n",
    "![image](https://github.com/user-attachments/assets/9863ebb3-07ba-43a8-90d5-104701d08365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42bae7c-e7fb-4e67-8340-4856de3f3aed",
   "metadata": {},
   "source": [
    "* O problema de XOR (ou OU exclusivo) é um problema clássico em redes neurais e aprendizado de máquina. Ele se refere à tentativa de treinar um perceptron (um modelo de rede neural simples) para aprender a função XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76051ebb-46e8-4bec-812c-68c0c71245d0",
   "metadata": {},
   "source": [
    "## Tabela \n",
    "\n",
    "![image](https://github.com/user-attachments/assets/0e25d1ba-0417-42eb-bce0-b72253e1757c)\n",
    "\n",
    "\n",
    "## A lógica do XOR é simples:\n",
    "\n",
    "* Retorna 1 se apenas uma das entradas for 1.\n",
    "* Retorna 0 caso contrário.\n",
    "\n",
    "## Passos \n",
    "\n",
    "1) Inicializa os pesos e os dados de entrada.\n",
    "2) Passagem direta (forward pass) para calcular a saída.\n",
    "3) Cálculo do erro e retropropagação do gradiente para ajustar os pesos.\n",
    "4) Atualização dos pesos usando o gradiente descendente.\n",
    "\n",
    "## 1) importe a biblioteca numpy \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "*  Será usado para operações matemáticas, como multiplicação de matrizes e cálculo da função de ativação.\n",
    "\n",
    "## 2) Definição da Função de Ativação Sigmoid\n",
    "\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "* A função sigmoid é uma função de ativação usada para transformar um valor numérico em um número entre 0 e 1.\n",
    "* Isso é útil porque podemos interpretar esse valor como uma probabilidade.\n",
    "\n",
    "## Exemplo: \n",
    "\n",
    "```bash\n",
    "sigmoid(0)  # Resultado: 0.5\n",
    "sigmoid(2)  # Resultado: 0.88 (próximo de 1)\n",
    "sigmoid(-2) # Resultado: 0.12 (próximo de 0)\n",
    "````\n",
    "\n",
    "## 3) Derivada da Função Sigmoid\n",
    "\n",
    "```python\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "```\n",
    "* A derivada da função sigmoid é usada no backpropagation para calcular os ajustes necessários nos pesos.\n",
    "* A fórmula vem da regra da derivação da função sigmoide.\n",
    "\n",
    "## Exemplo \n",
    "\n",
    "```bash\n",
    "x = sigmoid(2)  # 0.88\n",
    "sigmoid_derivative(x)  # 0.105 (menor quando x está próximo de 0 ou 1)\n",
    "```\n",
    "\n",
    "## 4) Criando os Dados de Entrada (X) e Saída (y)\n",
    "\n",
    "```python\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]])  # Problema do XOR\n",
    "```\n",
    "\n",
    "* X é a matriz de entrada com 4 exemplos (linhas) e 2 características (colunas).\n",
    "* y é a saída esperada, baseada na tabela verdade do XOR.\n",
    "\n",
    "## Interpretação:\n",
    "\n",
    "* A entrada (0,0) deve produzir saída 0\n",
    "* A entrada (0,1) deve produzir saída 1\n",
    "* A entrada (1,0) deve produzir saída 1\n",
    "* A entrada (1,1) deve produzir saída 0 (operação XOR)\n",
    "\n",
    "## 5) Inicialização dos Pesos\n",
    "\n",
    "```python\n",
    "np.random.seed(42)  # Para garantir resultados reproduzíveis\n",
    "pesos_entrada_oculta = np.random.uniform(-1, 1, (2, 5))  # 2 entradas -> 5 neurônios ocultos\n",
    "pesos_oculta_saida = np.random.uniform(-1, 1, (5, 1))    # 5 neurônios ocultos -> 1 saída\n",
    "```\n",
    "\n",
    "* Os pesos das conexões são inicializados aleatoriamente no intervalo [-1, 1].\n",
    "* pesos_entrada_oculta: Matriz (2x5) (2 neurônios de entrada → 5 neurônios na camada oculta)\n",
    "* pesos_oculta_saida: Matriz (5x1) (5 neurônios da camada oculta → 1 neurônio na saída)\n",
    "\n",
    "## Exemplo de pesos gerados:\n",
    "\n",
    "```python\n",
    "pesos_entrada_oculta =\n",
    "[[-0.25, 0.45, -0.12],\n",
    " [0.78, -0.56, 0.34]]\n",
    "\n",
    "pesos_oculta_saida =\n",
    "[[-0.68],\n",
    " [0.22],\n",
    " [-0.89]]\n",
    "```\n",
    "\n",
    "## 6) Definição da Taxa de Aprendizado\n",
    "\n",
    "```python\n",
    "learning_rate = 0.3\n",
    "```\n",
    "* Taxa de aprendizado controla o tamanho dos ajustes feitos nos pesos durante o treinamento.\n",
    "* Valores muito grandes podem fazer o treinamento divergir, e valores muito pequenos podem torná-lo muito lento.\n",
    "\n",
    "\n",
    "## 7) Loop de Treinamento\n",
    "\n",
    "```python\n",
    "for epoch in range(20000):\n",
    "```\n",
    "* O modelo será treinado por 20.000 iterações.\n",
    "\n",
    "## Forward Pass (Passagem Direta)\n",
    "\n",
    "```python\n",
    "camada_oculta = sigmoid(np.dot(X, pesos_entrada_oculta))  # Entrada -> Oculta\n",
    "camada_saida = sigmoid(np.dot(camada_oculta, pesos_oculta_saida))  # Oculta -> Saída\n",
    "```\n",
    "\n",
    "* np.dot(X, pesos_entrada_oculta): Multiplicação de matriz das entradas com os pesos da camada oculta.\n",
    "* np.dot(camada_oculta, pesos_oculta_saida): Multiplicação de matriz dos neurônios ocultos com os pesos da saída.\n",
    "* As ativações são passadas pela função sigmoide.\n",
    "\n",
    "## Exemplo de Forward Pass: \n",
    "\n",
    "* Suponha que X = [0, 1] e que os pesos iniciais sejam aleatórios.\n",
    "\n",
    "```python\n",
    "np.dot([0, 1], [[-0.5, 0.2, 0.7], [0.4, -0.3, 0.8]])\n",
    "# Resultado: [0.4, -0.3, 0.8]  # Sinais da camada oculta\n",
    "\n",
    "sigmoid([0.4, -0.3, 0.8]) \n",
    "# Resultado: [0.60, 0.42, 0.69]  # Ativação dos neurônios ocultos\n",
    "\n",
    "np.dot([0.60, 0.42, 0.69], [[-0.3], [0.5], [-0.6]])\n",
    "# Resultado: [-0.32]  # Sinal da camada de saída\n",
    "\n",
    "sigmoid(-0.32) \n",
    "# Resultado: 0.42  # Saída final (ainda com erro)\n",
    "```\n",
    "\n",
    "## Cálculo do Erro\n",
    "\n",
    "```python\n",
    "erro = y - camada_saida\n",
    "```\n",
    "* O erro é a diferença entre a saída esperada (y) e a saída calculada.\n",
    "\n",
    "\n",
    "## Backward Pass (Backpropagation)\n",
    "\n",
    "```python\n",
    "d_saida = erro * sigmoid_derivative(camada_saida)  # Gradiente da saída\n",
    "erro_oculta = d_saida.dot(pesos_oculta_saida.T)  # Propagação do erro para a camada oculta\n",
    "d_oculta = erro_oculta * sigmoid_derivative(camada_oculta)  # Gradiente da camada oculta\n",
    "```\n",
    "* Gradiente da saída: Calculamos quanto a saída precisa mudar.\n",
    "* Erro da camada oculta: Multiplicamos o erro da saída pelos pesos da camada oculta.\n",
    "* Gradiente da camada oculta: Calculamos quanto os neurônios ocultos devem ser ajustados.\n",
    "\n",
    "## Atualização dos Pesos\n",
    "\n",
    "```python\n",
    "pesos_oculta_saida += camada_oculta.T.dot(d_saida) * learning_rate\n",
    "pesos_entrada_oculta += X.T.dot(d_oculta) * learning_rate\n",
    "```\n",
    "\n",
    "* Os pesos são atualizados com base nos gradientes calculados.\n",
    "* Multiplicação matricial garante que todas as conexões sejam ajustadas corretamente.\n",
    "\n",
    "## Monitoramento do Erro\n",
    "\n",
    "```python\n",
    "if epoch % 1000 == 0:\n",
    "    print(f\"Erro na época {epoch}: {np.mean(np.abs(erro)):.4f}\")\n",
    "```\n",
    "\n",
    "* A cada 1000 épocas, o erro médio absoluto é impresso.\n",
    "* Isso permite verificar se o modelo está aprendendo corretamente.\n",
    "\n",
    "## Saída\n",
    "\n",
    "![image](https://github.com/user-attachments/assets/90bfff9e-2071-4242-a268-4c8aaf836074)\n",
    "\n",
    "## Usando a função de ativação Tanh\n",
    "\n",
    "```python\n",
    "# Função de ativação tanh e sua derivada\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - x**2\n",
    "```\n",
    "\n",
    "## Forward Pass \n",
    "\n",
    "```python\n",
    "camada_oculta = tanh(np.dot(X, pesos_entrada_oculta))  # Entrada -> Oculta\n",
    "camada_saida = tanh(np.dot(camada_oculta, pesos_oculta_saida))  # Oculta -> Saída\n",
    "```\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "```python\n",
    "d_saida = erro * tanh_derivative(camada_saida)  # Gradiente da saída\n",
    "erro_oculta = d_saida.dot(pesos_oculta_saida.T)  # Propagação do erro para a camada oculta\n",
    "d_oculta = erro_oculta * tanh_derivative(camada_oculta)  # Gradiente da oculta\n",
    "```\n",
    "\n",
    "## Saída \n",
    "\n",
    "![image](https://github.com/user-attachments/assets/256cc567-6fc4-4479-af80-b16e1e52014e)\n",
    "\n",
    "\n",
    "## Usando tensorflow / keras\n",
    "\n",
    "* <strong> Bibliotecas </strong>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "```\n",
    "\n",
    "## Definição dos dados de entrada (X) e saída esperada (y)\n",
    "\n",
    "```python\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]], dtype=np.float32)\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)  # XOR\n",
    "```\n",
    "\n",
    "## Construção do modelo com Keras\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(4, input_dim=2, activation='tanh'),  # Camada oculta com 4 neurônios\n",
    "    Dense(1, activation='sigmoid')  # Camada de saída com 1 neurônio\n",
    "])\n",
    "```\n",
    "\n",
    "## Compilação do modelo\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "## Treinamento do modelo\n",
    "\n",
    "```python\n",
    "model.fit(X, y, epochs=1000, verbose=0)  # Treina por 1000 épocas\n",
    "```\n",
    "\n",
    "## Testando o modelo treinado\n",
    "\n",
    "```python\n",
    "print(\"\\nSaídas previstas pelo modelo após treinamento:\")\n",
    "predictions = model.predict(X)\n",
    "print(np.round(predictions))  # Arredondamos para 0 ou 1\n",
    "```\n",
    "\n",
    "## Saída \n",
    "\n",
    "![image](https://github.com/user-attachments/assets/867befcf-edb9-4140-8371-4e7c4dfc5e2e)\n",
    "\n",
    "\n",
    "## Referências \n",
    "\n",
    "<strong>Livro: Redes Neurais Artificiais Para Engenharia e Ciências Aplicadas. Fundamentos Teóricos e Aspectos Práticos </strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e915b8-5cbe-4964-84fe-b4b482c96d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro na época 0: 0.4965\n",
      "Erro na época 1000: 0.0092\n",
      "Erro na época 2000: 0.0062\n",
      "Erro na época 3000: 0.0050\n",
      "Erro na época 4000: 0.0043\n",
      "Erro na época 5000: 0.0038\n",
      "Erro na época 6000: 0.0034\n",
      "Erro na época 7000: 0.0032\n",
      "Erro na época 8000: 0.0029\n",
      "Erro na época 9000: 0.0028\n",
      "Erro na época 10000: 0.0026\n",
      "Erro na época 11000: 0.0025\n",
      "Erro na época 12000: 0.0024\n",
      "Erro na época 13000: 0.0023\n",
      "Erro na época 14000: 0.0022\n",
      "Erro na época 15000: 0.0021\n",
      "Erro na época 16000: 0.0020\n",
      "Erro na época 17000: 0.0020\n",
      "Erro na época 18000: 0.0019\n",
      "Erro na época 19000: 0.0019\n",
      "\n",
      "Saída final da rede após treinamento:\n",
      "[[0.00000000e+00]\n",
      " [9.96359175e-01]\n",
      " [9.96417702e-01]\n",
      " [2.83999840e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Função de ativação tanh e sua derivada\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - x**2\n",
    "\n",
    "# Dados de entrada (4 exemplos, 2 características)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Saída esperada\n",
    "y = np.array([[0], [1], [1], [0]])  # Problema do XOR\n",
    "\n",
    "\n",
    "# Inicializa pesos aleatórios\n",
    "np.random.seed(42)\n",
    "pesos_entrada_oculta = np.random.uniform(-1, 1, (2, 5))  # 2 entradas -> 5 neurônios ocultos\n",
    "pesos_oculta_saida = np.random.uniform(-1, 1, (5, 1))    # 5 neurônios ocultos -> 1 saída\n",
    "\n",
    "\n",
    "# Taxa de aprendizado\n",
    "learning_rate = 0.3\n",
    "\n",
    "# Treinamento por 20.000 iterações\n",
    "for epoch in range(20000):\n",
    "    # Forward Pass\n",
    "    camada_oculta = tanh(np.dot(X, pesos_entrada_oculta))  # Entrada -> Oculta\n",
    "    camada_saida = tanh(np.dot(camada_oculta, pesos_oculta_saida))  # Oculta -> Saída\n",
    "\n",
    "    # Erro\n",
    "    erro = y - camada_saida\n",
    "\n",
    "    # Backpropagation\n",
    "    d_saida = erro * tanh_derivative(camada_saida)  # Gradiente da saída\n",
    "    erro_oculta = d_saida.dot(pesos_oculta_saida.T)  # Propagação do erro para a camada oculta\n",
    "    d_oculta = erro_oculta * tanh_derivative(camada_oculta)  # Gradiente da oculta\n",
    "\n",
    "    # Atualização dos pesos\n",
    "    pesos_oculta_saida += camada_oculta.T.dot(d_saida) * learning_rate\n",
    "    pesos_entrada_oculta += X.T.dot(d_oculta) * learning_rate\n",
    "\n",
    "    # Exibir erro a cada 1000 iterações\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Erro na época {epoch}: {np.mean(np.abs(erro)):.4f}\")\n",
    "\n",
    "# Teste após treinamento\n",
    "print(\"\\nSaída final da rede após treinamento:\")\n",
    "print(camada_saida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78c064-4fe8-4eae-ba81-24c54cdfb127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "jupyterlab-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
